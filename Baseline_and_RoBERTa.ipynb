{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f5d736aa3a546d5ab97227ea6451ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fced2c9f299247fdbbff9761f25b4a8a",
              "IPY_MODEL_666f7ec1410e45389390c802e8fe1c4d",
              "IPY_MODEL_94c68a5207cc4f3a964f41446fce26ca"
            ],
            "layout": "IPY_MODEL_54710005093c46e9bc456ce8bca11f67"
          }
        },
        "fced2c9f299247fdbbff9761f25b4a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d2847a4b52488189790bc21026ab0e",
            "placeholder": "​",
            "style": "IPY_MODEL_c681dfae8fc84a26b2ef1e57b582e202",
            "value": "Downloading: 100%"
          }
        },
        "666f7ec1410e45389390c802e8fe1c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5330d4e2b224757b5c442ea4cfbafd9",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ceb2ca21f554998b48d8d7e93e7054a",
            "value": 898823
          }
        },
        "94c68a5207cc4f3a964f41446fce26ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_873de6cc498949b58a01998a9f04649c",
            "placeholder": "​",
            "style": "IPY_MODEL_9b08564c5d5442658537ec706c79252b",
            "value": " 899k/899k [00:01&lt;00:00, 907kB/s]"
          }
        },
        "54710005093c46e9bc456ce8bca11f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d2847a4b52488189790bc21026ab0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c681dfae8fc84a26b2ef1e57b582e202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5330d4e2b224757b5c442ea4cfbafd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ceb2ca21f554998b48d8d7e93e7054a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "873de6cc498949b58a01998a9f04649c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b08564c5d5442658537ec706c79252b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b9c90ac5bb0466384fc8751878b29e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efe3263d2d94407f8a8d5fc979431f31",
              "IPY_MODEL_94549635d856468f8ba301c30d47a2e6",
              "IPY_MODEL_01e8f8769a984806a31bc02ea9f698c5"
            ],
            "layout": "IPY_MODEL_5e0df2c03fe04ac08360f633583d27ea"
          }
        },
        "efe3263d2d94407f8a8d5fc979431f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a6a0435a7f44300bbeefe9515436bb5",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c4eb4221b348fe81e263079a9befdf",
            "value": "Downloading: 100%"
          }
        },
        "94549635d856468f8ba301c30d47a2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e494e1e8fe49d980c62790d444c22c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9428a10288ca45cda5d0cc5144efb152",
            "value": 456318
          }
        },
        "01e8f8769a984806a31bc02ea9f698c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e35d4e659c8473094ad8b2b2d5b55bc",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa4aa7615824cfcbf2efbb566957cfc",
            "value": " 456k/456k [00:01&lt;00:00, 575kB/s]"
          }
        },
        "5e0df2c03fe04ac08360f633583d27ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6a0435a7f44300bbeefe9515436bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c4eb4221b348fe81e263079a9befdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57e494e1e8fe49d980c62790d444c22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9428a10288ca45cda5d0cc5144efb152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e35d4e659c8473094ad8b2b2d5b55bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa4aa7615824cfcbf2efbb566957cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c35fa17913483eb9a585616e4fea37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a48bb6a8ebf840cca8655cd0c3e17d06",
              "IPY_MODEL_3675cbc4016e43e29aedc7b931f4942b",
              "IPY_MODEL_a17d66d5274440d8a1f6377a55080fc8"
            ],
            "layout": "IPY_MODEL_092b6d01bdcf4a17b1d9cff2760a2650"
          }
        },
        "a48bb6a8ebf840cca8655cd0c3e17d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ade3ab7199c4d6084eee0f87e314b02",
            "placeholder": "​",
            "style": "IPY_MODEL_372d7e433e4942d6b6ef60f351a145c6",
            "value": "Downloading: 100%"
          }
        },
        "3675cbc4016e43e29aedc7b931f4942b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f921034954a401e973a018604f099c0",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47b074e2aa684068a32c324faebafc08",
            "value": 481
          }
        },
        "a17d66d5274440d8a1f6377a55080fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3816f13bd8a447b0bcc79b9415c1100a",
            "placeholder": "​",
            "style": "IPY_MODEL_58ae75835d3544b4a06305e4eee49fc2",
            "value": " 481/481 [00:00&lt;00:00, 26.1kB/s]"
          }
        },
        "092b6d01bdcf4a17b1d9cff2760a2650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ade3ab7199c4d6084eee0f87e314b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372d7e433e4942d6b6ef60f351a145c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f921034954a401e973a018604f099c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b074e2aa684068a32c324faebafc08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3816f13bd8a447b0bcc79b9415c1100a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58ae75835d3544b4a06305e4eee49fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d28965e036475c856fe1de63dc4f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ca6c50563174064b1bfbf82789d583b",
              "IPY_MODEL_6b4aae1d27cf4bc1a142ba911a9caa41",
              "IPY_MODEL_54979fa5f4414d50bb0612b87fe9bd1d"
            ],
            "layout": "IPY_MODEL_0564275db7384f5fa5406666b5dd8e4b"
          }
        },
        "6ca6c50563174064b1bfbf82789d583b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_791f9ee49ae943ce91caff38cb8fc433",
            "placeholder": "​",
            "style": "IPY_MODEL_37d717f003a44636ad30526d2f1a93bc",
            "value": "Downloading: 100%"
          }
        },
        "6b4aae1d27cf4bc1a142ba911a9caa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e47b5e39638b4edfa3f6a386e670eb63",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d60a705493314896828892c60cb2225f",
            "value": 501200538
          }
        },
        "54979fa5f4414d50bb0612b87fe9bd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e59194faecd4c1eb9518fa6e5f29bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba6d525bd21484582c079bbf6d4b371",
            "value": " 501M/501M [00:06&lt;00:00, 75.6MB/s]"
          }
        },
        "0564275db7384f5fa5406666b5dd8e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "791f9ee49ae943ce91caff38cb8fc433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d717f003a44636ad30526d2f1a93bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e47b5e39638b4edfa3f6a386e670eb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60a705493314896828892c60cb2225f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e59194faecd4c1eb9518fa6e5f29bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba6d525bd21484582c079bbf6d4b371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The reimplementation code was adapted from the publicly available code from Mekala et al. [1]"
      ],
      "metadata": {
        "id": "uKDmrU5NP0NC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h87oGlJAba1c",
        "outputId": "9a79f9e2-68c6-4d42-dba2-2cb2d9517117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3867M  100 3867M    0     0  12.4M      0  0:05:10  0:05:10 --:--:-- 18.9M\n"
          ]
        }
      ],
      "source": [
        "!curl -Lo prev.zip https://figshare.com/ndownloader/files/27188972"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip prev.zip   -d ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfdWHoGzb3ri",
        "outputId": "b401f40e-64e0-45e8-c4ee-3c5a6f1f8bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  prev.zip\n",
            " extracting: ./publish/__init__.py   \n",
            "   creating: ./publish/data/\n",
            "  inflating: ./publish/data/P1-Golden.xlsx  \n",
            "  inflating: ./publish/data/P2-Golden.xlsx  \n",
            "  inflating: ./publish/data_loader.py  \n",
            "  inflating: ./publish/evaluate.py   \n",
            "  inflating: ./publish/ML Baseline.ipynb  \n",
            "  inflating: ./publish/models.py     \n",
            "   creating: ./publish/models/\n",
            "  inflating: ./publish/models/p1-bert.bin  \n",
            "  inflating: ./publish/models/p1-bert-config.json  \n",
            "  inflating: ./publish/models/p1-elmo.bin  \n",
            "  inflating: ./publish/models/p1-fasttext.bin  \n",
            "  inflating: ./publish/models/p2-bert.bin  \n",
            "  inflating: ./publish/models/p2-bert-config.json  \n",
            "  inflating: ./publish/models/p2-elmo.bin  \n",
            "  inflating: ./publish/models/p2-fasttext.bin  \n",
            "   creating: ./publish/models/pre-trained/\n",
            "  inflating: ./publish/models/pre-trained/crawl-300d-2M.vec  \n",
            "  inflating: ./publish/models/pre-trained/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json  \n",
            "  inflating: ./publish/models/pre-trained/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5  \n",
            "  inflating: ./publish/p1_train_with_bert.py  \n",
            "  inflating: ./publish/p1_train_with_elmo.py  \n",
            "  inflating: ./publish/p1_train_with_fasttext.py  \n",
            "  inflating: ./publish/p2_train_with_bert.py  \n",
            "  inflating: ./publish/p2_train_with_elmo.py  \n",
            "  inflating: ./publish/p2_train_with_fasttext.py  \n",
            "  inflating: ./publish/preprocessing.py  \n",
            "  inflating: ./publish/train.py      \n",
            "  inflating: ./publish/utilities.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXQswkIzf7UZ",
        "outputId": "eafeebeb-018b-45d3-9a62-222fc626f7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YM061nxIgQuN",
        "outputId": "f118be01-5cdd-4d99-867d-451501153a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[K     |████████████████████████████████| 730 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.3.6)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 50.0 MB/s \n",
            "\u001b[?25hCollecting base58>=2.1.1\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.7.0)\n",
            "Collecting transformers<4.21,>=4.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting pytest>=6.2.5\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 13.6 MB/s \n",
            "\u001b[?25hCollecting fairscale==0.4.6\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 55.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.8/dist-packages (from allennlp) (4.64.1)\n",
            "Collecting lmdb>=1.2.1\n",
            "  Downloading lmdb-1.4.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting spacy<3.4,>=2.1.0\n",
            "  Downloading spacy-3.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 69.7 MB/s \n",
            "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 69.6 MB/s \n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting torch<1.13.0,>=1.10.0\n",
            "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 1.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.21.6)\n",
            "Collecting filelock<3.8,>=3.3\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.7)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.8/dist-packages (from allennlp) (1.7.3)\n",
            "Collecting torchvision<0.14.0,>=0.8.1\n",
            "  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting h5py>=3.6.0\n",
            "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.8/dist-packages (from allennlp) (5.6.0)\n",
            "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.8/dist-packages (from allennlp) (0.11.1)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from allennlp) (3.19.6)\n",
            "Collecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 46.0 MB/s \n",
            "\u001b[?25hCollecting rich<13.0,>=12.1\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.26.29-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 79.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.5.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.29\n",
            "  Downloading botocore-1.29.29-py3-none-any.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 73.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.29->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.15.0)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.57.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from pytest>=6.2.5->allennlp) (22.1.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->allennlp) (2022.9.24)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
            "Collecting typer>=0.4.1\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.10)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp) (5.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 63.9 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.0.1)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools, sacremoses\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307251 sha256=3c0816522c0d18c3648b400e5b3cd0828097154b2bf68f4d0321aa57045e80d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/4c/a4/f6c0eec2ec5c8ffca075e62b0329801f862e1f1b71422f456b\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=b827c9221825c274ccf7f0881f7c3082b1a77ac1a1fcd848da80da0f65dd3f32\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp38-cp38-linux_x86_64.whl size=3996107 sha256=2ef349497e18e5bb57c2646266456f2a45ee8b34c9e6dda5198498c1d09bbf8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/ec/56/de861aae102c449ade2378772abbf9eb7e9acfe9a80f3e6036\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f8e59cf1a313ffecf3de5c29a460a3eb17cda80572543e0a861f402427313229\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=98ef88bf5b32f5c8b00af6382bc2b826c6325290c8aadf3a6d6fb6e5bbdff5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built fairscale termcolor jsonnet pathtools sacremoses\n",
            "Installing collected packages: urllib3, requests, jmespath, smmap, botocore, typer, s3transfer, pydantic, gitdb, filelock, commonmark, torch, tokenizers, thinc, shortuuid, setproctitle, sentry-sdk, rich, pluggy, pathtools, iniconfig, huggingface-hub, GitPython, exceptiongroup, docker-pycreds, boto3, wandb, transformers, torchvision, termcolor, tensorboardX, spacy, sentencepiece, sacremoses, pytest, lmdb, jsonnet, h5py, fairscale, cached-path, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.11.1\n",
            "    Uninstalling huggingface-hub-0.11.1:\n",
            "      Successfully uninstalled huggingface-hub-0.11.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.25.1\n",
            "    Uninstalling transformers-4.25.1:\n",
            "      Successfully uninstalled transformers-4.25.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.1.1\n",
            "    Uninstalling termcolor-2.1.1:\n",
            "      Successfully uninstalled termcolor-2.1.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.3\n",
            "    Uninstalling spacy-3.4.3:\n",
            "      Successfully uninstalled spacy-3.4.3\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: lmdb\n",
            "    Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.12.1 which is incompatible.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 allennlp-2.10.1 base58-2.1.1 boto3-1.26.29 botocore-1.29.29 cached-path-1.1.6 commonmark-0.9.1 docker-pycreds-0.4.0 exceptiongroup-1.0.4 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.10 h5py-3.7.0 huggingface-hub-0.10.1 iniconfig-1.1.1 jmespath-1.0.1 jsonnet-0.19.1 lmdb-1.4.0 pathtools-0.1.2 pluggy-1.0.0 pydantic-1.8.2 pytest-7.2.0 requests-2.28.1 rich-12.6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.11.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 spacy-3.3.1 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 urllib3-1.26.13 wandb-0.12.21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "filelock",
                  "h5py",
                  "huggingface_hub",
                  "requests",
                  "termcolor",
                  "tokenizers",
                  "torch",
                  "transformers",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k35s3vhtgrEO",
        "outputId": "fd030e7b-7cee-419f-9903-0b6873e20e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3134045 sha256=b4c7c7e43c90c667c79182a444a792a9f90d09105944db2cacb9dd50d570b7ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "5q1n1_84hEMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_X_original = pd.read_csv(\"./data/original_data_X_train.csv\")\n",
        "train_y_original = pd.read_csv(\"./data/original_data_y_train.csv\")\n",
        "augmented_data = pd.read_csv(\"./data/balanced_augmentation_dataset.csv\")\n",
        "X_test = pd.read_csv(\"./data/original_data_X_test.csv\")\n",
        "y_test = pd.read_csv(\"./data/original_data_y_test.csv\")\n",
        "\n",
        "augmented_data=augmented_data[[\"Reviews\", \"Useful?\"]]\n",
        "augmented_data.rename(columns={\"Reviews\":\"reviews\", \"Useful?\":\"Judgement\"}, inplace=True)\n",
        "X_train_augmented = pd.concat([train_X_original, augmented_data[[\"reviews\"]]], axis=0, ignore_index=True)\n",
        "y_train_augmented = pd.concat([train_y_original, augmented_data[[\"Judgement\"]]], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "95xbr2yzhFw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "x8vKHhp5dqtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "\n",
        "    sentences = dataset.reviews.values\n",
        "    labels = np.array(dataset.Judgement)\n",
        "\n",
        "    # Load the BERT tokenizer.\n",
        "    logging.info(\"Loading BERT tokenizer...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "    logging.info(f\"Original: {sentences[0]}\")\n",
        "    logging.info(f\"Tokenized: {tokenizer.tokenize(sentences[0])}\")\n",
        "    logging.info(\n",
        "        f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))}\"\n",
        "    )\n",
        "\n",
        "    max_len = 0\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "        max_len = max(max_len, len(input_ids))\n",
        "\n",
        "    logging.info(f\"Max sentence length: {max_len}\")\n",
        "\n",
        "    # Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,  # Sentence to encode.\n",
        "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "            max_length=max_len + 10,  # Pad & truncate all sentences.\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,  # Construct attn. masks.\n",
        "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
        "            truncation=True,\n",
        "        )\n",
        "        # Add the encoded sentence to the list.\n",
        "        input_ids.append(encoded_dict[\"input_ids\"])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    # Print sentence 0, now as a list of IDs.\n",
        "    logging.info(f\"Original: {sentences[0]}\")\n",
        "    logging.info(f\"Token IDs:{input_ids[0]}\")\n",
        "    logging.info(f\"Length of labels: {len(labels)}\")\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "\n",
        "def get_loader(input_ids, attention_masks, labels, batch_size=32, loader_type=\"TRAIN\"):\n",
        "    \"\"\"\n",
        "    Returns a dataloader for the dataset.\n",
        "\n",
        "    The DataLoader needs to know our batch size for training.\n",
        "    For fine-tuning BERT on a specific task, the authors recommend a batch\n",
        "    size of 16 or 32.\n",
        "    \"\"\"\n",
        "    # Combine the training inputs into a TensorDataset.\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    if loader_type == \"TRAIN\":\n",
        "        # We'll take training samples in random order.\n",
        "        dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler=RandomSampler(dataset),  # Select batches randomly\n",
        "            batch_size=batch_size,  # Trains with this batch size.\n",
        "        )\n",
        "    else:\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        dataloader = DataLoader(\n",
        "            dataset,  # The validation samples.\n",
        "            sampler=SequentialSampler(dataset),  # Pull out batches sequentially.\n",
        "            batch_size=batch_size,  # Evaluate with this batch size.\n",
        "        )\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "yjHkILJ7dw_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "ij9VSRG8evbY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTvePgMuqLmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import torch\n",
        "from dateutil import tz\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from allennlp.modules.elmo import batch_to_ids\n",
        "from transformers import BertTokenizer\n",
        "from datetime import timedelta\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    \"\"\"\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    \"\"\"\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        logging.info(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "        logging.info(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        logging.info(\"No GPU available, using the CPU instead.\")\n",
        "        device = torch.device(\"cpu\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def current_utc_time():\n",
        "    return datetime.datetime.now().astimezone(tz.tzutc())\n",
        "\n",
        "\n",
        "# https://github.com/ylhsieh/pytorch-elmo-classification\n",
        "class Corpus(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentences,\n",
        "        labels,\n",
        "        lowercase=False,\n",
        "        test_size=0.05,\n",
        "        max_len=-1,\n",
        "        label_dict=None,\n",
        "        shuffle=True,\n",
        "    ):\n",
        "        self.shuffle = shuffle\n",
        "        self.lowercase = lowercase\n",
        "        self.all_sentences = sentences\n",
        "        self.all_labels = labels\n",
        "        self.max_len = max_len\n",
        "        self.label_dict = label_dict\n",
        "        self.test_size = test_size\n",
        "        self.test_sentences = []\n",
        "        self.test_labels = []\n",
        "        self.train_sentences = []\n",
        "        self.train_labels = []\n",
        "\n",
        "        self.split_dataset()\n",
        "        self.tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "        self.train = self.tokenize(self.train_sentences, self.train_labels)\n",
        "        self.test = self.tokenize(self.test_sentences, self.test_labels)\n",
        "\n",
        "    def split_dataset(self):\n",
        "        if self.shuffle:\n",
        "            data = list(zip(self.all_sentences, self.all_labels))\n",
        "            random.shuffle(data)\n",
        "            self.all_sentences, self.all_labels = zip(*data)\n",
        "\n",
        "        ind = int(self.test_size * len(self.all_sentences))\n",
        "        self.test_sentences = self.all_sentences[:ind]\n",
        "        self.test_labels = self.all_labels[:ind]\n",
        "        self.train_sentences = self.all_sentences[ind:]\n",
        "        self.train_labels = self.all_labels[ind:]\n",
        "\n",
        "    def tokenize(self, sentences, labels):\n",
        "        processed_sentences = []\n",
        "        processed_labels = []\n",
        "        cropped = 0\n",
        "        for (sent, label) in zip(sentences, labels):\n",
        "            if self.lowercase:\n",
        "                sent = sent.lower().strip()\n",
        "            else:\n",
        "                sent = sent.strip()\n",
        "            sent = self.tokenizer.tokenize(sent)\n",
        "            if self.max_len > 0:\n",
        "                if len(sent) > self.max_len:\n",
        "                    cropped += 1\n",
        "                sent = sent[: self.max_len]\n",
        "            if self.label_dict:\n",
        "                label = self.label_dict[label]\n",
        "            processed_sentences.append(sent)\n",
        "            processed_labels.append(label)\n",
        "        print(f\"Number of sentences cropped: {cropped}\")\n",
        "\n",
        "        return list(zip(processed_labels, processed_sentences))\n",
        "\n",
        "\n",
        "def batchify(data, bsz, shuffle=False, gpu=False):\n",
        "    if shuffle:\n",
        "        random.shuffle(data)\n",
        "    tags, sents = zip(*data)\n",
        "    nbatch = (len(sents) + bsz - 1) // bsz\n",
        "    # downsample biggest class\n",
        "    # sents, tags = balance_tags(sents, tags)\n",
        "\n",
        "    for i in range(nbatch):\n",
        "\n",
        "        batch = sents[i * bsz : (i + 1) * bsz]\n",
        "        batch_tags = tags[i * bsz : (i + 1) * bsz]\n",
        "        # lengths = [len(x) for x in batch]\n",
        "        # sort items by length (decreasing)\n",
        "        # batch, batch_tags, lengths = length_sort(batch, batch_tags, lengths)\n",
        "\n",
        "        # Pad batches to maximum sequence length in batch\n",
        "        # find length to pad to\n",
        "\n",
        "        # maxlen = lengths[0]\n",
        "        # for b_i in range(len(batch)):\n",
        "        #     pads = [pad_id] * (maxlen-len(batch[b_i]))\n",
        "        #     batch[b_i] = batch[b_i] + pads\n",
        "        # batch = torch.tensor(batch).long()\n",
        "        batch = batch_to_ids(batch)\n",
        "        batch_tags = torch.tensor(batch_tags).long()\n",
        "        # lengths = [torch.tensor(l).long() for l in lengths]\n",
        "\n",
        "        # yield (batch, batch_tags, lengths)\n",
        "        yield batch, batch_tags\n"
      ],
      "metadata": {
        "id": "7NUXA7aBewfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "SCdvMUDVePJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from utilities import get_device\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    logging.info(\"Starting Evaluation...\")\n",
        "    print(\"Starting Evaluation...\")\n",
        "\n",
        "    # Tracking variables\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    # Predict\n",
        "    for batch in dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(\n",
        "                b_input_ids, token_type_ids=None, attention_mask=b_input_mask\n",
        "            )\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to(\"cpu\").numpy()\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "    logging.info(\"    DONE.\")\n",
        "    print(\"    DONE.\")\n",
        "\n",
        "    truth = np.hstack((true_labels))\n",
        "    a = np.vstack((predictions))\n",
        "    pred = np.argmax(a, axis=1)\n",
        "    logging.info(f\"Prediction: {pred}\")\n",
        "    logging.info(f\"Truth: {truth}\")\n",
        "\n",
        "    report = classification_report(truth, pred, output_dict=True)\n",
        "\n",
        "    logging.info(f\"Classification Report: \\n {report}\")\n",
        "    print(f\"Classification Report: \\n {report}\")\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def evaluate_elmo(model, data, criterion_ce, gpu=False):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    all_accuracies = 0.0\n",
        "    all_loss = 0.0\n",
        "    nbatches = 0.0\n",
        "\n",
        "    for batch in data:\n",
        "        nbatches += 1.0\n",
        "        source, tags = batch\n",
        "        if gpu:\n",
        "            source = source.to(\"cuda\")\n",
        "            tags = tags.to(\"cuda\")\n",
        "        # output = model(source, lengths)\n",
        "        output = model(source)\n",
        "        v_loss = criterion_ce(output, tags)\n",
        "        max_vals, max_indices = torch.max(output, -1)\n",
        "\n",
        "        accuracy = torch.mean(max_indices.eq(tags).float()).item()\n",
        "        all_accuracies += accuracy\n",
        "        all_loss += v_loss\n",
        "    return all_accuracies / nbatches, all_loss / nbatches\n"
      ],
      "metadata": {
        "id": "LSuJzmTgeOqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "IlqtdFxKeeTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "from allennlp.modules.elmo import Elmo\n",
        "\n",
        "\n",
        "def get_model(name: str = \"BERT\", **kwargs):\n",
        "\n",
        "    if name == \"BERT\":\n",
        "        # Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "        # linear classification layer on top.\n",
        "        num_labels = kwargs.get(\"num_labels\", 2)\n",
        "        output_attentions = kwargs.get(\"output_attentions\", False)\n",
        "        output_hidden_states = kwargs.get(\"output_hidden_states\", False)\n",
        "\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\",\n",
        "            num_labels=num_labels,  # The number of output labels--2 for binary classification.\n",
        "            # You can increase this for multi-class tasks.\n",
        "            output_attentions=output_attentions,  # Whether the model returns attentions weights.\n",
        "            output_hidden_states=output_hidden_states,  # Whether the model returns all hidden-states.\n",
        "        )\n",
        "        params = list(model.named_parameters())\n",
        "        logging.info(\n",
        "            \"The BERT model has {:} different named parameters.\\n\".format(len(params))\n",
        "        )\n",
        "\n",
        "        logging.info(\"==== Embedding Layer ====\\n\")\n",
        "\n",
        "        for p in params[0:5]:\n",
        "            logging.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        logging.info(\"\\n==== First Transformer ====\\n\")\n",
        "\n",
        "        for p in params[5:21]:\n",
        "            logging.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "        logging.info(\"\\n==== Output Layer ====\\n\")\n",
        "\n",
        "        for p in params[-4:]:\n",
        "            logging.info(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "        return model\n",
        "\n",
        "    elif name == \"ELMO\":\n",
        "        num_labels = kwargs.get(\"num_labels\", 2)\n",
        "        on_gpu = kwargs.get(\"on_gpu\", True)\n",
        "        dropout = kwargs.get(\"dropout\", 0.5)\n",
        "        model = SimpleELMOClassifier(\n",
        "            label_size=num_labels, use_gpu=on_gpu, dropout=dropout\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "class SimpleELMOClassifier(nn.Module):\n",
        "    def __init__(self, label_size, use_gpu, dropout):\n",
        "        super(SimpleELMOClassifier, self).__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        self.dropout = dropout\n",
        "        options_file = \"./models/pre-trained/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json\"\n",
        "        weight_file = \"./models/pre-trained/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5\"\n",
        "        self.elmo = Elmo(\n",
        "            options_file, weight_file, 1, dropout=dropout, do_layer_norm=False\n",
        "        )\n",
        "        # elmo output\n",
        "        #         Dict with keys:\n",
        "        #         ``'elmo_representations'``: ``List[torch.Tensor]``\n",
        "        #             A ``num_output_representations`` list of ELMo representations for the input sequence.\n",
        "        #             Each representation is shape ``(batch_size, timesteps, embedding_dim)``\n",
        "        #         ``'mask'``:  ``torch.Tensor``\n",
        "        #             Shape ``(batch_size, timesteps)`` long tensor with sequence mask.\n",
        "        self.conv1 = nn.Conv1d(1024, 16, 3)\n",
        "        self.p1 = nn.AdaptiveMaxPool1d(128)\n",
        "        self.activation_func = nn.ReLU6()\n",
        "        self.dropout_l = nn.Dropout(dropout)\n",
        "        self.hidden2label = nn.Linear(2048, label_size)\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.hidden2label.named_parameters():\n",
        "            if \"bias\" in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif \"weight\" in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "        for name, param in self.conv1.named_parameters():\n",
        "            if \"bias\" in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "            elif \"weight\" in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        elmo_out = self.elmo(sentences)\n",
        "        x = elmo_out[\"elmo_representations\"][0]\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation_func(x)\n",
        "        x = self.p1(x)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.dropout_l(x)\n",
        "        y = self.hidden2label(x)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "N8rwpsAued36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "hZelPokFelxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import digits\n",
        "import re\n",
        "\n",
        "\n",
        "# convert text to lower case\n",
        "def to_lowercase(x):\n",
        "    if type(x) is str:\n",
        "        return x.lower()\n",
        "    elif x is None:\n",
        "        return str(None)\n",
        "\n",
        "\n",
        "# remove urls (e.g. www.sap.com https:)\n",
        "def remove_urls(x):\n",
        "    URL_FAST_REGEX = \"(((https?|ftp|file):\\/\\/)|www\\\\.)\\\\S+\"\n",
        "    return re.sub(URL_FAST_REGEX, \" \", x)\n",
        "\n",
        "\n",
        "# remove tags (e.g. < br> <html>)\n",
        "def remove_tags(x):\n",
        "    FILTER_ALL_TAGS = \"<[^>]*>\"\n",
        "    return re.sub(FILTER_ALL_TAGS, \" \", x)\n",
        "\n",
        "\n",
        "# remove special characters (eg. ' ! Ã £ Ã ¢ { })\n",
        "def filter_characters_regex(x):\n",
        "    return re.sub(r\"[^a-zA-Z0-9,.']+\", \" \", x)\n",
        "\n",
        "\n",
        "def numbers_and_mail(txt):\n",
        "    # Remove Number and Email Id\n",
        "    pretxt = re.sub(r\"\\S*@\\S*\\s?|\\w*\\d\\w*\", \"\", str(txt))\n",
        "    # pretxt = re.sub(r'[\\w\\.,]+@[\\w\\.,]+\\.\\w+','' ,txt)\n",
        "    remove_digits = str.maketrans(\"\", \"\", digits)\n",
        "    pretxt = pretxt.translate(remove_digits)\n",
        "    return pretxt\n",
        "\n",
        "\n",
        "# remove multiple whitespaces\n",
        "def remove_multiple_white_spaces(x):\n",
        "    FILTER_MULTIPLE_WHITESPACES = \"\\s\\s+\"\n",
        "    return re.sub(FILTER_MULTIPLE_WHITESPACES, \" \", x)\n",
        "\n",
        "\n",
        "# strip spaces at the end and the starting\n",
        "def strip_white_spaces(x):\n",
        "    return x.strip()\n",
        "\n",
        "\n",
        "# invoke all above functions\n",
        "def preprocess(x):\n",
        "    return strip_white_spaces(\n",
        "        remove_multiple_white_spaces(\n",
        "            numbers_and_mail(\n",
        "                filter_characters_regex(remove_tags(remove_urls(to_lowercase(x))))\n",
        "            )\n",
        "        )\n",
        "    )\n"
      ],
      "metadata": {
        "id": "pKkD72-6eNzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "q15jo_3NenqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "#from utilities import format_time, get_device, flat_accuracy\n",
        "import logging\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    validation_dataloader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    seed=42,\n",
        "    epochs=5,\n",
        "):\n",
        "    \"\"\"\n",
        "    This training code is based on the `run_glue.py` script here:\n",
        "    https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "    \"\"\"\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = seed\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # We'll store a number of quantities such as training and validation loss,\n",
        "    # validation accuracy, and timings.\n",
        "    training_stats = []\n",
        "\n",
        "    # Measure the total training time for the whole run.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "\n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        logging.info(\"\")\n",
        "        logging.info(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
        "        logging.info(\"Training...\")\n",
        "        print(\"\")\n",
        "        print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
        "        print(\"Training...\")\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Put the model into training mode. Don't be mislead--the call to\n",
        "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "        # `dropout` and `batchnorm` layers behave differently during training\n",
        "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 5 == 0 and not step == 0:\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "\n",
        "                # Report progress.\n",
        "                logging.info(\n",
        "                    \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(\n",
        "                        step, len(train_dataloader), elapsed\n",
        "                    )\n",
        "                )\n",
        "                print(\n",
        "                    \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.\".format(\n",
        "                        step, len(train_dataloader), elapsed\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Unpack this training batch from our dataloader.\n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "            # `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids\n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. PyTorch doesn't do this automatically because\n",
        "            # accumulating the gradients is \"convenient while training RNNs\".\n",
        "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # It returns different numbers of parameters depending on what arguments\n",
        "            # arge given and what flags are set. For our useage here, it returns\n",
        "            # the loss (because we provided labels) and the \"logits\"--the model\n",
        "            # outputs prior to activation.\n",
        "            output = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=b_input_mask,\n",
        "                labels=b_labels,\n",
        "            )\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value\n",
        "            # from the tensor.\n",
        "            total_train_loss += output.loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            output.loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        logging.info(\"\")\n",
        "        logging.info(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        #logging.info(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        #print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        logging.info(\"\")\n",
        "        logging.info(\"Running Validation...\")\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "\n",
        "            # Unpack this training batch from our dataloader.\n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids\n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which\n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                # The documentation for this `model` function is here:\n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                # values prior to applying an activation function like the softmax.\n",
        "                output = model(\n",
        "                    b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels,\n",
        "                )\n",
        "\n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += output.loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = output.logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to(\"cpu\").numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        logging.info(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "\n",
        "        logging.info(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        logging.info(\"  Validation took: {:}\".format(validation_time))\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                \"epoch\": epoch_i + 1,\n",
        "                \"Training Loss\": avg_train_loss,\n",
        "                \"Valid. Loss\": avg_val_loss,\n",
        "                \"Valid. Accur.\": avg_val_accuracy,\n",
        "                #\"Training Time\": training_time,\n",
        "                \"Validation Time\": validation_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    logging.info(\"\")\n",
        "    logging.info(\"Training complete!\")\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    logging.info(\n",
        "        \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\n",
        "    )\n",
        "    print(\n",
        "        \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\n",
        "    )\n",
        "    return model, training_stats\n",
        "\n",
        "\n",
        "def train_elmo(classifier, train_batch, optimizer_, criterion_ce, gpu=False, clip=5.0):\n",
        "    classifier.train()\n",
        "    classifier.zero_grad()\n",
        "    # source, tags, lengths = train_batch\n",
        "    source, tags = train_batch\n",
        "    if gpu:\n",
        "        source = source.to(\"cuda\")\n",
        "        tags = tags.to(\"cuda\")\n",
        "\n",
        "    # output: batch x nclasses\n",
        "    # output = classifier(source, lengths)\n",
        "    output = classifier(source)\n",
        "    c_loss = criterion_ce(output, tags)\n",
        "\n",
        "    c_loss.backward()\n",
        "\n",
        "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
        "    torch.nn.utils.clip_grad_norm_(classifier.parameters(), clip)\n",
        "    optimizer_.step()\n",
        "\n",
        "    total_loss = c_loss.item()\n",
        "\n",
        "    # probs = F.softmax(output, dim=-1)\n",
        "    # max_vals, max_indices = torch.max(probs, -1)\n",
        "    # accuracy = torch.mean(max_indices.eq(tags).float()).item()\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "evL0DjcDetOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YG6hNjb6e1Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "dBM-IKwadKts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before augmentation"
      ],
      "metadata": {
        "id": "vAQqRC7xjZXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import pandas as pd\n",
        "from imp import reload\n",
        "import pickle\n",
        "import fasttext\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import datetime\n",
        "\n",
        "reload(logging)\n",
        "\n",
        "# Parameters\n",
        "seed = 42\n",
        "epochs = 25\n",
        "batch_size = 16\n",
        "learning_rate = 4e-5\n",
        "#golden_1 = pd.read_excel(\"./data/P1-Golden.xlsx\")\n",
        "golden_1 = pd.concat([train_X_original, train_y_original], axis=1)\n",
        "VECTORS_FILEPATH = \"./publish/models/pre-trained/crawl-300d-2M.vec\"\n",
        "SAVE_MODEL = True\n",
        "output_dir = \"./publish/models/\"\n",
        "\n",
        "# Set up log file\n",
        "logging.basicConfig(\n",
        "    filename=f\"{os.getcwd()}/fasttext-p1.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    logging.info(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "    logging.info(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    logging.info(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Prepare dataset\n",
        "# Shuffle the DataFrame rows\n",
        "#golden_1 = golden_1.sample(frac=1)\n",
        "\n",
        "fasttext_input = []\n",
        "fasttext_label = []\n",
        "for i, row in golden_1.iterrows():\n",
        "    text = row[\"reviews\"]\n",
        "    label = \"__label__\" + str(row[\"Judgement\"])\n",
        "    fasttext_label.append(label)\n",
        "    fasttext_input.append(label + \" \" + text)\n",
        "\n",
        "golden_1[\"fasttext_input\"] = fasttext_input\n",
        "golden_1[\"fasttext_label\"] = fasttext_label\n",
        "\n",
        "# Split the dataframe into 95:5 train:validation splits\n",
        "train_file = open(\"./data/p1.fasttext.train\", \"w\", encoding=\"utf-8\")\n",
        "validation_rows = []\n",
        "# ind = int((len(golden_1)*95)/100)\n",
        "for i, row in golden_1.iterrows():\n",
        "    if True:\n",
        "        train_file.write(row[\"fasttext_input\"] + \"\\n\")\n",
        "    # else:\n",
        "    #     validation_rows.append((row[\"Judgement\"], row[\"reviews\"]))\n",
        "train_file.close()\n",
        "validation = pd.concat([X_test, y_test], axis=1)\n",
        "for i, row in validation.iterrows():\n",
        "  validation_rows.append((row[\"Judgement\"], row[\"reviews\"]))\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "training_stats = []\n",
        "start_time = datetime.now()\n",
        "\n",
        "model = fasttext.train_supervised(input=\"./data/p1.fasttext.train\", lr=learning_rate, epoch=epochs,\n",
        "                          wordNgrams=2, bucket=200000, dim=300, loss='hs',\n",
        "                          pretrainedVectors=VECTORS_FILEPATH)\n",
        "train_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "predicted = []\n",
        "y_test = []\n",
        "eval_time_start = datetime.now()\n",
        "\n",
        "for (l, r) in validation_rows:\n",
        "    y_test.append(l)\n",
        "    prediction = int(model.predict(r.strip(\"\\n\"))[0][0].split(\"_\")[-1])\n",
        "    predicted.append(prediction)\n",
        "\n",
        "eval_report = classification_report(y_test, predicted, output_dict=True)\n",
        "eval_time = (datetime.now() - eval_time_start)\n",
        "\n",
        "training_stats.append(\n",
        "    {\"evaluation_report\": eval_report, \"train_time\": train_time, \"eval_time\": eval_time}\n",
        ")\n",
        "logging.info(f\"Training Stats: \\n {training_stats}\")\n",
        "print(f\"Evaluation Report: \\n {eval_report}\")\n",
        "\n",
        "# Save report\n",
        "with open(\"fasttext-p1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(training_stats, f)\n",
        "\n",
        "if SAVE_MODEL:\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "\n",
        "    # Save a trained model.\n",
        "    # They can then be reloaded using fasttext.load_model(PATH)\n",
        "    model.save_model(f\"{output_dir}/p1-fasttext.bin\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAsSXBnxdP0u",
        "outputId": "dcf298fd-dc73-40aa-e142-a289d492f3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Report: \n",
            " {'0': {'precision': 0.92, 'recall': 0.6571428571428571, 'f1-score': 0.7666666666666667, 'support': 105}, '1': {'precision': 0.712, 'recall': 0.9368421052631579, 'f1-score': 0.8090909090909091, 'support': 95}, 'accuracy': 0.79, 'macro avg': {'precision': 0.8160000000000001, 'recall': 0.7969924812030076, 'f1-score': 0.7878787878787878, 'support': 200}, 'weighted avg': {'precision': 0.8212, 'recall': 0.79, 'f1-score': 0.7868181818181819, 'support': 200}}\n",
            "Saving model to ./publish/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After augmentation"
      ],
      "metadata": {
        "id": "Z_uGRfakmQuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import pandas as pd\n",
        "from imp import reload\n",
        "import pickle\n",
        "import fasttext\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import datetime\n",
        "\n",
        "reload(logging)\n",
        "\n",
        "# Parameters\n",
        "seed = 42\n",
        "epochs = 25\n",
        "batch_size = 16\n",
        "learning_rate = 4e-5\n",
        "#golden_1 = pd.read_excel(\"./data/P1-Golden.xlsx\")\n",
        "golden_1 = pd.concat([X_train_augmented, y_train_augmented], axis=1)\n",
        "VECTORS_FILEPATH = \"./publish/models/pre-trained/crawl-300d-2M.vec\"\n",
        "SAVE_MODEL = True\n",
        "output_dir = \"./publish/models/\"\n",
        "\n",
        "# Set up log file\n",
        "logging.basicConfig(\n",
        "    filename=f\"{os.getcwd()}/fasttext-p1.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    logging.info(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "    logging.info(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    logging.info(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Prepare dataset\n",
        "# Shuffle the DataFrame rows\n",
        "#golden_1 = golden_1.sample(frac=1)\n",
        "\n",
        "fasttext_input = []\n",
        "fasttext_label = []\n",
        "for i, row in golden_1.iterrows():\n",
        "    text = row[\"reviews\"]\n",
        "    label = \"__label__\" + str(row[\"Judgement\"])\n",
        "    fasttext_label.append(label)\n",
        "    fasttext_input.append(label + \" \" + text)\n",
        "\n",
        "golden_1[\"fasttext_input\"] = fasttext_input\n",
        "golden_1[\"fasttext_label\"] = fasttext_label\n",
        "\n",
        "# Split the dataframe into 95:5 train:validation splits\n",
        "train_file = open(\"./data/p1.fasttext.train\", \"w\", encoding=\"utf-8\")\n",
        "validation_rows = []\n",
        "# ind = int((len(golden_1)*95)/100)\n",
        "for i, row in golden_1.iterrows():\n",
        "    if True:\n",
        "        train_file.write(row[\"fasttext_input\"] + \"\\n\")\n",
        "    # else:\n",
        "    #     validation_rows.append((row[\"Judgement\"], row[\"reviews\"]))\n",
        "train_file.close()\n",
        "validation = pd.concat([X_test, y_test], axis=1)\n",
        "for i, row in validation.iterrows():\n",
        "  validation_rows.append((row[\"Judgement\"], row[\"reviews\"]))\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "training_stats = []\n",
        "start_time = datetime.now()\n",
        "\n",
        "model = fasttext.train_supervised(input=\"./data/p1.fasttext.train\", lr=learning_rate, epoch=epochs,\n",
        "                          wordNgrams=2, bucket=200000, dim=300, loss='hs',\n",
        "                          pretrainedVectors=VECTORS_FILEPATH)\n",
        "train_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "predicted = []\n",
        "y_test = []\n",
        "eval_time_start = datetime.now()\n",
        "\n",
        "for (l, r) in validation_rows:\n",
        "    y_test.append(l)\n",
        "    prediction = int(model.predict(r.strip(\"\\n\"))[0][0].split(\"_\")[-1])\n",
        "    predicted.append(prediction)\n",
        "\n",
        "eval_report = classification_report(y_test, predicted, output_dict=True)\n",
        "eval_time = (datetime.now() - eval_time_start)\n",
        "\n",
        "training_stats.append(\n",
        "    {\"evaluation_report\": eval_report, \"train_time\": train_time, \"eval_time\": eval_time}\n",
        ")\n",
        "logging.info(f\"Training Stats: \\n {training_stats}\")\n",
        "print(f\"Evaluation Report: \\n {eval_report}\")\n",
        "\n",
        "# Save report\n",
        "with open(\"fasttext-p1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(training_stats, f)\n",
        "\n",
        "if SAVE_MODEL:\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "\n",
        "    # Save a trained model.\n",
        "    # They can then be reloaded using fasttext.load_model(PATH)\n",
        "    model.save_model(f\"{output_dir}/p1-fasttext.bin\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_9wNlqhmQQP",
        "outputId": "63bd764e-e988-4d41-c9fa-f15994ce968e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Report: \n",
            " {'0': {'precision': 0.8484848484848485, 'recall': 0.8, 'f1-score': 0.823529411764706, 'support': 105}, '1': {'precision': 0.7920792079207921, 'recall': 0.8421052631578947, 'f1-score': 0.8163265306122449, 'support': 95}, 'accuracy': 0.82, 'macro avg': {'precision': 0.8202820282028203, 'recall': 0.8210526315789474, 'f1-score': 0.8199279711884755, 'support': 200}, 'weighted avg': {'precision': 0.8216921692169217, 'recall': 0.82, 'f1-score': 0.820108043217287, 'support': 200}}\n",
            "Saving model to ./publish/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ELMO"
      ],
      "metadata": {
        "id": "7FQ4EfxQdJHz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqRAoIC_oNdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import pandas as pd\n",
        "from imp import reload\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "#from models import get_model\n",
        "#from utilities import Corpus, batchify\n",
        "#from train import train_elmo\n",
        "#from evaluate import evaluate_elmo\n",
        "import collections\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "\n",
        "reload(logging)\n",
        "\n",
        "# Parameters\n",
        "seed = 42\n",
        "epochs = 25\n",
        "batch_size = 16\n",
        "learning_rate = 2e-4\n",
        "# golden_1 = pd.read_excel(\"./data/P1-Golden.xlsx\")\n",
        "golden_1 = pd.concat([train_X_original, train_y_original], axis=1)\n",
        "CUDA = torch.cuda.is_available()\n",
        "SAVE_MODEL = True\n",
        "output_dir = \"./publish/models/\"\n",
        "\n",
        "# Set up log file\n",
        "logging.basicConfig(\n",
        "    filename=f\"{os.getcwd()}/elmo-p1.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "if CUDA:\n",
        "    device = torch.device(\"cuda\")\n",
        "    logging.info(f\"There are {torch.cuda.device_count()} GPU(s) available.\")\n",
        "    logging.info(f\"We will use the GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    logging.info(\"No GPU available, using the CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Prepare dataset\n",
        "\n",
        "corpus = Corpus(sentences=list(golden_1.reviews), labels=list(golden_1.Judgement))\n",
        "# Print corpus stats\n",
        "class_counts = collections.Counter([c[0] for c in corpus.train])\n",
        "print(\"Train: {}\".format(class_counts))\n",
        "class_counts = collections.Counter([c[0] for c in corpus.test])\n",
        "print(\"Test: {}\".format(class_counts))\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size, shuffle=True)\n",
        "test_data = batchify(corpus.test, batch_size, shuffle=False)\n",
        "\n",
        "print(\"Loaded data!\")\n",
        "\n",
        "# Model\n",
        "model = get_model(\"ELMO\").to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler\n",
        "learning_rate_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "# Loss\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize weights\n",
        "model.init_weights()\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "training_stats = []\n",
        "niter_global = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    logging.info(f\"Epoch {epoch}/{epochs}...\")\n",
        "    print(f\"Epoch {epoch}/{epochs}...\")\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # loop through all batches in training data\n",
        "    train_loss = 0\n",
        "    nbatches = 0\n",
        "    for train_batch in train_data:\n",
        "        loss = train_elmo(model, train_batch, optimizer, criterion_ce, gpu=CUDA)\n",
        "        train_loss += loss\n",
        "        niter_global += 1\n",
        "        nbatches += 1\n",
        "        if niter_global % 10 == 0:\n",
        "            msg = \"Train loss {:.5f}\".format(loss)\n",
        "            print(msg)\n",
        "            logging.info(msg)\n",
        "\n",
        "    train_loss = train_loss / nbatches\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    with torch.no_grad():\n",
        "        accuracy, v_loss = evaluate_elmo(model, test_data, criterion_ce, gpu=CUDA)\n",
        "    msg = \"val acc {:.4f}, val loss {:.4f}\".format(accuracy, v_loss)\n",
        "    print(msg)\n",
        "    logging.info(msg)\n",
        "    # we use generator, so must re-gen test data\n",
        "    test_data = batchify(corpus.test, batch_size, shuffle=False)\n",
        "\n",
        "    # clear cache between epoch\n",
        "    torch.cuda.empty_cache()\n",
        "    # decay learning rate\n",
        "    learning_rate_scheduler.step()\n",
        "    # shuffle between epochs\n",
        "    train_data = batchify(corpus.train, batch_size, shuffle=True)\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"Training Loss\": train_loss,\n",
        "            \"Valid. Loss\": v_loss,\n",
        "            \"Valid. Accur.\": accuracy,\n",
        "        }\n",
        "    )\n",
        "\n",
        "train_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "model.eval()\n",
        "y_test = []\n",
        "predictions = []\n",
        "eval_time_start = datetime.now()\n",
        "\n",
        "for batch in test_data:\n",
        "    source, labels = batch\n",
        "    y_test += labels.tolist()\n",
        "    if CUDA:\n",
        "        source = source.to(\"cuda\")\n",
        "    output = model(source)\n",
        "    _, max_indices = torch.max(output, -1)\n",
        "    predictions += max_indices.tolist()\n",
        "\n",
        "eval_report = classification_report(y_test, predictions, output_dict=True)\n",
        "eval_time = (datetime.now() - eval_time_start).total_seconds()\n",
        "\n",
        "training_stats.append({\"evaluation_report\": eval_report, \"train_time\": train_time, \"eval_time\": eval_time,})\n",
        "logging.info(f\"Training Stats: \\n {training_stats}\")\n",
        "print(f\"Evaluation Report: \\n {eval_report}\")\n",
        "\n",
        "# Save report\n",
        "with open(\"elmo-p1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(training_stats, f)\n",
        "\n",
        "if SAVE_MODEL:\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "\n",
        "    # Save a trained model.\n",
        "    # They can then be reloaded using:\n",
        "    # model = TheModelClass(*args, **kwargs)\n",
        "    # model.load_state_dict(torch.load(PATH))\n",
        "    torch.save(model.state_dict(), f\"{output_dir}/p1-elmo.bin\")\n"
      ],
      "metadata": {
        "id": "6BIVqaqYdQ2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "9EUvdnd1dK40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before Augmentation"
      ],
      "metadata": {
        "id": "sPKlNR_yophO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_original = pd.read_csv(\"./data/original_data_X_train.csv\")\n",
        "train_y_original = pd.read_csv(\"./data/original_data_y_train.csv\")\n",
        "augmented_data = pd.read_csv(\"./data/balanced_augmentation_dataset.csv\")\n",
        "X_test = pd.read_csv(\"./data/original_data_X_test.csv\")\n",
        "y_test = pd.read_csv(\"./data/original_data_y_test.csv\")\n",
        "\n",
        "augmented_data=augmented_data[[\"Reviews\", \"Useful?\"]]\n",
        "augmented_data.rename(columns={\"Reviews\":\"reviews\", \"Useful?\":\"Judgement\"}, inplace=True)\n",
        "X_train_augmented = pd.concat([train_X_original, augmented_data[[\"reviews\"]]], axis=0, ignore_index=True)\n",
        "y_train_augmented = pd.concat([train_y_original, augmented_data[[\"Judgement\"]]], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "m2nS8KuVopSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "#from utilities import get_device, current_utc_time\n",
        "import pandas as pd\n",
        "from imp import reload\n",
        "#from data_loader import get_loader, prepare_dataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "#from models import get_model\n",
        "#from train import train_model\n",
        "#from evaluate import evaluate_model\n",
        "import pickle\n",
        "#from datetime import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "\n",
        "reload(logging)\n",
        "\n",
        "# Parameters\n",
        "model_name = \"BERT\"\n",
        "seed = 0\n",
        "epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 2e-4\n",
        "epsilon = 1e-8\n",
        "#golden_1 = pd.read_excel(\"./data/P1-Golden.xlsx\")\n",
        "golden_1 = pd.concat([train_X_original, train_y_original], axis=1)\n",
        "validation = pd.concat([X_test, y_test], axis=1)\n",
        "SAVE_MODEL = True\n",
        "output_dir = \"./publish/models/\"\n",
        "\n",
        "# Set up log file\n",
        "#current_time = current_utc_time()\n",
        "logging.basicConfig(\n",
        "    filename=f\"{os.getcwd()}/bert-p1.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "device = get_device()\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Prepare dataset\n",
        "all_input_ids, all_attention_masks, all_labels = prepare_dataset(golden_1)\n",
        "val_input_ids, val_attention_masks, val_labels = prepare_dataset(validation)\n",
        "\n",
        "# Shuffle data and separate evaluation dataset\n",
        "print(\"Separating test and train data\")\n",
        "# while True:\n",
        "#     indices = np.arange(all_input_ids.shape[0])\n",
        "#     np.random.shuffle(indices)\n",
        "#     all_input_ids = all_input_ids[indices]\n",
        "#     all_attention_masks = all_attention_masks[indices]\n",
        "#     all_labels = all_labels[indices]\n",
        "#     val_labels = all_labels[:50]\n",
        "\n",
        "#     # Ensure that we do not have too much bias in validation dataset\n",
        "#     bias_ratio = np.count_nonzero(val_labels == 1) / np.count_nonzero(val_labels == 0)\n",
        "#     if 0.9 < bias_ratio < 1.1:\n",
        "#         val_input_ids = all_input_ids[:50]\n",
        "#         val_attention_masks = all_attention_masks[:50]\n",
        "#         break\n",
        "\n",
        "val_dataloader = get_loader(\n",
        "    val_input_ids, val_attention_masks, val_labels, batch_size=batch_size, loader_type=\"VALIDATE\"\n",
        ")\n",
        "\n",
        "# input_ids = all_input_ids[50:]\n",
        "# attention_masks = all_attention_masks[50:]\n",
        "# labels = all_labels[50:]\n",
        "input_ids = all_input_ids\n",
        "attention_masks = all_attention_masks\n",
        "labels = all_labels\n",
        "\n",
        "logging.info(f\"Number of train samples: {len(input_ids)}\")\n",
        "logging.info(f\"Number of validation samples: {len(val_input_ids)}\")\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "start_time = datetime.now()\n",
        "\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "# Prepare dataloader\n",
        "train_dataloader = get_loader(input_ids, attention_masks, labels, batch_size=batch_size)\n",
        "\n",
        "# model\n",
        "model = get_model(model_name).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "    eps=epsilon,  # args.adam_epsilon  - default is 1e-8.\n",
        ")\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,  # Default value in run_glue.py\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "model, stats = train_model(\n",
        "    model, train_dataloader, val_dataloader, optimizer, scheduler, seed=seed, epochs=epochs\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "train_time = (datetime.now() - start_time).total_seconds()\n",
        "eval_time_start = datetime.now()\n",
        "eval_report = evaluate_model(model, val_dataloader)\n",
        "eval_time = (datetime.now() - eval_time_start).total_seconds()\n",
        "\n",
        "training_stats = {\n",
        "    \"train_size\": len(labels),\n",
        "    \"val_size\": len(val_labels),\n",
        "    \"training_stats\": stats,\n",
        "    \"evaluation_report\": eval_report,\n",
        "    \"train_time\": train_time,\n",
        "    \"eval_time\": eval_time,\n",
        "}\n",
        "\n",
        "logging.info(f\"Training Stats: \\n {training_stats}\")\n",
        "print(f\"Evaluation Report: \\n {eval_report}\")\n",
        "\n",
        "# Save report\n",
        "with open(\"bert-p1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(training_stats, f)\n",
        "\n",
        "if SAVE_MODEL:\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = (\n",
        "        model.module if hasattr(model, \"module\") else model\n",
        "    )  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITaob0pFdRSU",
        "outputId": "8551d86b-33a0-40db-996a-5685fa3dde80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separating test and train data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch     5  of     50.    Elapsed: 0:00:03.\n",
            "  Batch    10  of     50.    Elapsed: 0:00:06.\n",
            "  Batch    15  of     50.    Elapsed: 0:00:09.\n",
            "  Batch    20  of     50.    Elapsed: 0:00:12.\n",
            "  Batch    25  of     50.    Elapsed: 0:00:15.\n",
            "  Batch    30  of     50.    Elapsed: 0:00:18.\n",
            "  Batch    35  of     50.    Elapsed: 0:00:21.\n",
            "  Batch    40  of     50.    Elapsed: 0:00:24.\n",
            "  Batch    45  of     50.    Elapsed: 0:00:27.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:33 (h:mm:ss)\n",
            "Starting Evaluation...\n",
            "    DONE.\n",
            "Classification Report: \n",
            " {'0': {'precision': 0.9042553191489362, 'recall': 0.8095238095238095, 'f1-score': 0.8542713567839195, 'support': 105}, '1': {'precision': 0.8113207547169812, 'recall': 0.9052631578947369, 'f1-score': 0.8557213930348259, 'support': 95}, 'accuracy': 0.855, 'macro avg': {'precision': 0.8577880369329587, 'recall': 0.8573934837092732, 'f1-score': 0.8549963749093727, 'support': 200}, 'weighted avg': {'precision': 0.8601114010437576, 'recall': 0.855, 'f1-score': 0.8549601240031001, 'support': 200}}\n",
            "Evaluation Report: \n",
            " {'0': {'precision': 0.9042553191489362, 'recall': 0.8095238095238095, 'f1-score': 0.8542713567839195, 'support': 105}, '1': {'precision': 0.8113207547169812, 'recall': 0.9052631578947369, 'f1-score': 0.8557213930348259, 'support': 95}, 'accuracy': 0.855, 'macro avg': {'precision': 0.8577880369329587, 'recall': 0.8573934837092732, 'f1-score': 0.8549963749093727, 'support': 200}, 'weighted avg': {'precision': 0.8601114010437576, 'recall': 0.855, 'f1-score': 0.8549601240031001, 'support': 200}}\n",
            "Saving model to ./publish/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After augmentation"
      ],
      "metadata": {
        "id": "XlMjYirzzrIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_original = pd.read_csv(\"./data/original_data_X_train.csv\")\n",
        "train_y_original = pd.read_csv(\"./data/original_data_y_train.csv\")\n",
        "augmented_data = pd.read_csv(\"./data/balanced_augmentation_dataset.csv\")\n",
        "X_test = pd.read_csv(\"./data/original_data_X_test.csv\")\n",
        "y_test = pd.read_csv(\"./data/original_data_y_test.csv\")\n",
        "\n",
        "augmented_data=augmented_data[[\"Reviews\", \"Useful?\"]]\n",
        "augmented_data.rename(columns={\"Reviews\":\"reviews\", \"Useful?\":\"Judgement\"}, inplace=True)\n",
        "X_train_augmented = pd.concat([train_X_original, augmented_data[[\"reviews\"]]], axis=0, ignore_index=True)\n",
        "y_train_augmented = pd.concat([train_y_original, augmented_data[[\"Judgement\"]]], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "QdeMHzv3z1E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "#from utilities import get_device, current_utc_time\n",
        "import pandas as pd\n",
        "from imp import reload\n",
        "#from data_loader import get_loader, prepare_dataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "#from models import get_model\n",
        "#from train import train_model\n",
        "#from evaluate import evaluate_model\n",
        "import pickle\n",
        "#from datetime import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "\n",
        "reload(logging)\n",
        "\n",
        "# Parameters\n",
        "model_name = \"BERT\"\n",
        "seed = 0\n",
        "epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 2e-4\n",
        "epsilon = 1e-8\n",
        "#golden_1 = pd.read_excel(\"./data/P1-Golden.xlsx\")\n",
        "golden_1 = pd.concat([X_train_augmented, y_train_augmented], axis=1)\n",
        "validation = pd.concat([X_test, y_test], axis=1)\n",
        "SAVE_MODEL = True\n",
        "output_dir = \"./publish/models/\"\n",
        "\n",
        "# Set up log file\n",
        "#current_time = current_utc_time()\n",
        "logging.basicConfig(\n",
        "    filename=f\"{os.getcwd()}/bert-p1.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "device = get_device()\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Prepare dataset\n",
        "all_input_ids, all_attention_masks, all_labels = prepare_dataset(golden_1)\n",
        "val_input_ids, val_attention_masks, val_labels = prepare_dataset(validation)\n",
        "\n",
        "# Shuffle data and separate evaluation dataset\n",
        "print(\"Separating test and train data\")\n",
        "# while True:\n",
        "#     indices = np.arange(all_input_ids.shape[0])\n",
        "#     np.random.shuffle(indices)\n",
        "#     all_input_ids = all_input_ids[indices]\n",
        "#     all_attention_masks = all_attention_masks[indices]\n",
        "#     all_labels = all_labels[indices]\n",
        "#     val_labels = all_labels[:50]\n",
        "\n",
        "#     # Ensure that we do not have too much bias in validation dataset\n",
        "#     bias_ratio = np.count_nonzero(val_labels == 1) / np.count_nonzero(val_labels == 0)\n",
        "#     if 0.9 < bias_ratio < 1.1:\n",
        "#         val_input_ids = all_input_ids[:50]\n",
        "#         val_attention_masks = all_attention_masks[:50]\n",
        "#         break\n",
        "\n",
        "val_dataloader = get_loader(\n",
        "    val_input_ids, val_attention_masks, val_labels, batch_size=batch_size, loader_type=\"VALIDATE\"\n",
        ")\n",
        "\n",
        "# input_ids = all_input_ids[50:]\n",
        "# attention_masks = all_attention_masks[50:]\n",
        "# labels = all_labels[50:]\n",
        "input_ids = all_input_ids\n",
        "attention_masks = all_attention_masks\n",
        "labels = all_labels\n",
        "\n",
        "logging.info(f\"Number of train samples: {len(input_ids)}\")\n",
        "logging.info(f\"Number of validation samples: {len(val_input_ids)}\")\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "start_time = datetime.now()\n",
        "\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "\n",
        "# Prepare dataloader\n",
        "train_dataloader = get_loader(input_ids, attention_masks, labels, batch_size=batch_size)\n",
        "\n",
        "# model\n",
        "model = get_model(model_name).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "    eps=epsilon,  # args.adam_epsilon  - default is 1e-8.\n",
        ")\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,  # Default value in run_glue.py\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "model, stats = train_model(\n",
        "    model, train_dataloader, val_dataloader, optimizer, scheduler, seed=seed, epochs=epochs\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "train_time = (datetime.now() - start_time).total_seconds()\n",
        "eval_time_start = datetime.now()\n",
        "eval_report = evaluate_model(model, val_dataloader)\n",
        "eval_time = (datetime.now() - eval_time_start).total_seconds()\n",
        "\n",
        "training_stats = {\n",
        "    \"train_size\": len(labels),\n",
        "    \"val_size\": len(val_labels),\n",
        "    \"training_stats\": stats,\n",
        "    \"evaluation_report\": eval_report,\n",
        "    \"train_time\": train_time,\n",
        "    \"eval_time\": eval_time,\n",
        "}\n",
        "\n",
        "logging.info(f\"Training Stats: \\n {training_stats}\")\n",
        "print(f\"Evaluation Report: \\n {eval_report}\")\n",
        "\n",
        "# Save report\n",
        "with open(\"bert-p1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(training_stats, f)\n",
        "\n",
        "if SAVE_MODEL:\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = (\n",
        "        model.module if hasattr(model, \"module\") else model\n",
        "    )  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJVx2J1nc-p5",
        "outputId": "0518da50-14a1-4e2f-adc5-919956f8d088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separating test and train data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch     5  of    159.    Elapsed: 0:00:06.\n",
            "  Batch    10  of    159.    Elapsed: 0:00:12.\n",
            "  Batch    15  of    159.    Elapsed: 0:00:18.\n",
            "  Batch    20  of    159.    Elapsed: 0:00:24.\n",
            "  Batch    25  of    159.    Elapsed: 0:00:30.\n",
            "  Batch    30  of    159.    Elapsed: 0:00:36.\n",
            "  Batch    35  of    159.    Elapsed: 0:00:42.\n",
            "  Batch    40  of    159.    Elapsed: 0:00:48.\n",
            "  Batch    45  of    159.    Elapsed: 0:00:54.\n",
            "  Batch    50  of    159.    Elapsed: 0:01:00.\n",
            "  Batch    55  of    159.    Elapsed: 0:01:06.\n",
            "  Batch    60  of    159.    Elapsed: 0:01:12.\n",
            "  Batch    65  of    159.    Elapsed: 0:01:19.\n",
            "  Batch    70  of    159.    Elapsed: 0:01:25.\n",
            "  Batch    75  of    159.    Elapsed: 0:01:31.\n",
            "  Batch    80  of    159.    Elapsed: 0:01:37.\n",
            "  Batch    85  of    159.    Elapsed: 0:01:43.\n",
            "  Batch    90  of    159.    Elapsed: 0:01:50.\n",
            "  Batch    95  of    159.    Elapsed: 0:01:56.\n",
            "  Batch   100  of    159.    Elapsed: 0:02:02.\n",
            "  Batch   105  of    159.    Elapsed: 0:02:08.\n",
            "  Batch   110  of    159.    Elapsed: 0:02:14.\n",
            "  Batch   115  of    159.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    159.    Elapsed: 0:02:27.\n",
            "  Batch   125  of    159.    Elapsed: 0:02:33.\n",
            "  Batch   130  of    159.    Elapsed: 0:02:39.\n",
            "  Batch   135  of    159.    Elapsed: 0:02:45.\n",
            "  Batch   140  of    159.    Elapsed: 0:02:51.\n",
            "  Batch   145  of    159.    Elapsed: 0:02:57.\n",
            "  Batch   150  of    159.    Elapsed: 0:03:04.\n",
            "  Batch   155  of    159.    Elapsed: 0:03:10.\n",
            "\n",
            "  Average training loss: 0.65\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:03:17 (h:mm:ss)\n",
            "Starting Evaluation...\n",
            "    DONE.\n",
            "Classification Report: \n",
            " {'0': {'precision': 0.7833333333333333, 'recall': 0.8952380952380953, 'f1-score': 0.8355555555555555, 'support': 105}, '1': {'precision': 0.8625, 'recall': 0.7263157894736842, 'f1-score': 0.7885714285714286, 'support': 95}, 'accuracy': 0.815, 'macro avg': {'precision': 0.8229166666666667, 'recall': 0.8107769423558897, 'f1-score': 0.812063492063492, 'support': 200}, 'weighted avg': {'precision': 0.8209375, 'recall': 0.815, 'f1-score': 0.8132380952380952, 'support': 200}}\n",
            "Evaluation Report: \n",
            " {'0': {'precision': 0.7833333333333333, 'recall': 0.8952380952380953, 'f1-score': 0.8355555555555555, 'support': 105}, '1': {'precision': 0.8625, 'recall': 0.7263157894736842, 'f1-score': 0.7885714285714286, 'support': 95}, 'accuracy': 0.815, 'macro avg': {'precision': 0.8229166666666667, 'recall': 0.8107769423558897, 'f1-score': 0.812063492063492, 'support': 200}, 'weighted avg': {'precision': 0.8209375, 'recall': 0.815, 'f1-score': 0.8132380952380952, 'support': 200}}\n",
            "Saving model to ./publish/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-_cV8b2dAgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: Roberta"
      ],
      "metadata": {
        "id": "2L9-XnQhrj0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zybu2jIm0jkv",
        "outputId": "689b564a-cae2-4721-a7ea-60e3798b914d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 77.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from:\n",
        "\n",
        "https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=7Gpe9D1QHoCd"
      ],
      "metadata": {
        "id": "gXZRM2m1sn0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After augmentation"
      ],
      "metadata": {
        "id": "bKuVPDCkHAhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOA6wb-grlQG",
        "outputId": "f1b984ca-eda6-4ef2-d7f2-155642752334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_X_original = pd.read_csv(\"./data/original_data_X_train.csv\")\n",
        "train_y_original = pd.read_csv(\"./data/original_data_y_train.csv\")\n",
        "augmented_data = pd.read_csv(\"./data/balanced_augmentation_dataset.csv\")\n",
        "X_test = pd.read_csv(\"./data/original_data_X_test.csv\")\n",
        "y_test = pd.read_csv(\"./data/original_data_y_test.csv\")\n",
        "\n",
        "augmented_data=augmented_data[[\"Reviews\", \"Useful?\"]]\n",
        "augmented_data.rename(columns={\"Reviews\":\"reviews\", \"Useful?\":\"Judgement\"}, inplace=True)\n",
        "X_train_augmented = pd.concat([train_X_original, augmented_data[[\"reviews\"]]], axis=0, ignore_index=True)\n",
        "y_train_augmented = pd.concat([train_y_original, augmented_data[[\"Judgement\"]]], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "km8ZaAVFrlql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
      ],
      "metadata": {
        "id": "20YTrkpfo04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8f5d736aa3a546d5ab97227ea6451ef5",
            "fced2c9f299247fdbbff9761f25b4a8a",
            "666f7ec1410e45389390c802e8fe1c4d",
            "94c68a5207cc4f3a964f41446fce26ca",
            "54710005093c46e9bc456ce8bca11f67",
            "26d2847a4b52488189790bc21026ab0e",
            "c681dfae8fc84a26b2ef1e57b582e202",
            "e5330d4e2b224757b5c442ea4cfbafd9",
            "7ceb2ca21f554998b48d8d7e93e7054a",
            "873de6cc498949b58a01998a9f04649c",
            "9b08564c5d5442658537ec706c79252b",
            "4b9c90ac5bb0466384fc8751878b29e0",
            "efe3263d2d94407f8a8d5fc979431f31",
            "94549635d856468f8ba301c30d47a2e6",
            "01e8f8769a984806a31bc02ea9f698c5",
            "5e0df2c03fe04ac08360f633583d27ea",
            "0a6a0435a7f44300bbeefe9515436bb5",
            "a2c4eb4221b348fe81e263079a9befdf",
            "57e494e1e8fe49d980c62790d444c22c",
            "9428a10288ca45cda5d0cc5144efb152",
            "7e35d4e659c8473094ad8b2b2d5b55bc",
            "eaa4aa7615824cfcbf2efbb566957cfc",
            "b6c35fa17913483eb9a585616e4fea37",
            "a48bb6a8ebf840cca8655cd0c3e17d06",
            "3675cbc4016e43e29aedc7b931f4942b",
            "a17d66d5274440d8a1f6377a55080fc8",
            "092b6d01bdcf4a17b1d9cff2760a2650",
            "6ade3ab7199c4d6084eee0f87e314b02",
            "372d7e433e4942d6b6ef60f351a145c6",
            "6f921034954a401e973a018604f099c0",
            "47b074e2aa684068a32c324faebafc08",
            "3816f13bd8a447b0bcc79b9415c1100a",
            "58ae75835d3544b4a06305e4eee49fc2"
          ]
        },
        "outputId": "60c57a15-32b8-428b-cff9-a4376daec217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f5d736aa3a546d5ab97227ea6451ef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b9c90ac5bb0466384fc8751878b29e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c35fa17913483eb9a585616e4fea37"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.reviews\n",
        "        self.targets = self.data.Judgement\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "ycEOa9__rvgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_original[\"Judgement\"]=  train_y_original[\"Judgement\"].astype(int)\n",
        "y_test[\"Judgement\"]=  y_test[\"Judgement\"].astype(int)\n",
        "training_set = ReviewData(pd.concat([X_train_augmented, y_train_augmented], axis=1), tokenizer, MAX_LEN)\n",
        "testing_set = ReviewData(pd.concat([X_test, y_test], axis=1), tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "eKGNApoWr275"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "4aT7k6tps0ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "iD4l0mbds7_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "00d28965e036475c856fe1de63dc4f3e",
            "6ca6c50563174064b1bfbf82789d583b",
            "6b4aae1d27cf4bc1a142ba911a9caa41",
            "54979fa5f4414d50bb0612b87fe9bd1d",
            "0564275db7384f5fa5406666b5dd8e4b",
            "791f9ee49ae943ce91caff38cb8fc433",
            "37d717f003a44636ad30526d2f1a93bc",
            "e47b5e39638b4edfa3f6a386e670eb63",
            "d60a705493314896828892c60cb2225f",
            "3e59194faecd4c1eb9518fa6e5f29bc4",
            "9ba6d525bd21484582c079bbf6d4b371"
          ]
        },
        "id": "DaB2VoAks-7a",
        "outputId": "ce5803ac-8312-469c-d481-952b82572ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00d28965e036475c856fe1de63dc4f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClass(\n",
              "  (l1): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "m_2-qw-OtAC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "JfOd0EartGlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        #print(_, data)\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "   \n",
        "    return "
      ],
      "metadata": {
        "id": "r84QIJAEtHpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "310CxgB63Tqx",
        "outputId": "ac0988d2-7169-4c9e-cf76-4600070e517e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f622bd27220>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQnZdARmtJf7",
        "outputId": "645163f1-8307-49af-cf3a-ecd8ae242daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 1.6757196187973022\n",
            "Training Accuracy per 5000 steps: 25.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "317it [01:56,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 0: 78.67298578199052\n",
            "Training Loss Epoch: 0.5465499488901264\n",
            "Training Accuracy Epoch: 78.67298578199052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "            all_preds.append(big_idx.cpu().detach().numpy())\n",
        "            all_targets.append(targets.cpu().detach().numpy())\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    #print(all_preds, all_targets)\n",
        "    print(classification_report(all_targets, all_preds))\n",
        "    return epoch_accu\n"
      ],
      "metadata": {
        "id": "zUge_XkNtLOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "metadata": {
        "id": "EaW03XXvteN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2161c470-fb44-48a4-fe6e-15033fbf481f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 19.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss per 100 steps: 0.18931196630001068\n",
            "Validation Accuracy per 100 steps: 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:02, 17.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Epoch: 0.2876202328503132\n",
            "Validation Accuracy Epoch: 89.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90       105\n",
            "           1       0.90      0.87      0.89        95\n",
            "\n",
            "    accuracy                           0.90       200\n",
            "   macro avg       0.90      0.89      0.89       200\n",
            "weighted avg       0.90      0.90      0.89       200\n",
            "\n",
            "Accuracy on test data = 89.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before data augmentation:"
      ],
      "metadata": {
        "id": "BbrYiqZPvqlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = ReviewData( pd.concat([train_X_original, train_y_original], axis=1), tokenizer, MAX_LEN)\n",
        "testing_set = ReviewData(pd.concat([X_test, y_test], axis=1), tokenizer, MAX_LEN)\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "del tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "del model\n",
        "model = RobertaClass()\n",
        "model.to(device)\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V0uTqiztfOX",
        "outputId": "148895c0-929f-4bc1-fc3f-07bd74cc3be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 1.6566047668457031\n",
            "Training Accuracy per 5000 steps: 12.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:36,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 0: 68.75\n",
            "Training Loss Epoch: 0.8903598971664906\n",
            "Training Accuracy Epoch: 68.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0TGvN9qwQLL",
        "outputId": "bce237d7-7482-40ca-95bc-6d2067301864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 18.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss per 100 steps: 0.9771075248718262\n",
            "Validation Accuracy per 100 steps: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:02, 17.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Epoch: 0.3865887239575386\n",
            "Validation Accuracy Epoch: 87.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.78      0.86       105\n",
            "           1       0.80      0.97      0.88        95\n",
            "\n",
            "    accuracy                           0.87       200\n",
            "   macro avg       0.88      0.87      0.87       200\n",
            "weighted avg       0.89      0.87      0.87       200\n",
            "\n",
            "Accuracy on test data = 87.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Analysis"
      ],
      "metadata": {
        "id": "v1QizxEb2sPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (After data augmentation)"
      ],
      "metadata": {
        "id": "Dt_eRkGe23hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Blh0ag2thA",
        "outputId": "8964a241-a335-4dd1-cd6f-a44db8dbab51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 5000 steps: 0.16894738376140594\n",
            "Training Accuracy per 5000 steps: 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "317it [01:55,  2.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Total Accuracy for Epoch 0: 89.25750394944708\n",
            "Training Loss Epoch: 0.27435735361621205\n",
            "Training Accuracy Epoch: 89.25750394944708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_and_analyze(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_data = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "            all_preds.append(big_idx.cpu().detach().numpy())\n",
        "            all_targets.append(targets.cpu().detach().numpy())\n",
        "            all_data.append(tokenizer.batch_decode(data[\"ids\"]))\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "    all_data = np.concatenate(all_data)\n",
        "    #print(all_preds, all_targets)\n",
        "    print(classification_report(all_targets, all_preds))\n",
        "    return epoch_accu, all_preds, all_targets, all_data\n"
      ],
      "metadata": {
        "id": "3WnLcrk42twe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, preds, targets, input_sentences = valid_and_analyze(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgwGr1a82zn9",
        "outputId": "cbe6c2d4-766a-4084-d566-e3633f3b01f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 11.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss per 100 steps: 0.7252010703086853\n",
            "Validation Accuracy per 100 steps: 75.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:03, 16.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss Epoch: 0.2968002664297819\n",
            "Validation Accuracy Epoch: 89.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       105\n",
            "           1       0.89      0.89      0.89        95\n",
            "\n",
            "    accuracy                           0.90       200\n",
            "   macro avg       0.89      0.89      0.89       200\n",
            "weighted avg       0.90      0.90      0.90       200\n",
            "\n",
            "Accuracy on test data = 89.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_df = pd.DataFrame({'preds':preds, 'targets':targets, 'input_sent':input_sentences})\n",
        "debug_df = debug_df[debug_df[\"preds\"] != debug_df[\"targets\"]]\n"
      ],
      "metadata": {
        "id": "gJfKifO65amp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4293
        },
        "id": "oo0LnlJd58UJ",
        "outputId": "bdcc3c8b-b544-4286-da1f-b535169e1ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     preds  targets                                         input_sent\n",
              "3        0        1  <s>This app is wonderful! Quite a lifesaver. T...\n",
              "23       0        1  <s>With iPhone 5 works just perfectly</s><pad>...\n",
              "26       1        0  <s>Pac-Man 256 is now one of my favorite games...\n",
              "30       1        0  <s>It's ok. If you're going to charge for an a...\n",
              "34       1        0  <s>5 estrellas o can talk we my love one and s...\n",
              "48       0        1  <s>Great app. Love that I can scan several dif...\n",
              "52       0        1  <s>Love this app and it's gotten even better w...\n",
              "55       0        1  <s>I run GTD on my iPhone using the one note a...\n",
              "65       1        0  <s>Phenomenal!One of my most used apps. I have...\n",
              "81       0        1  <s>Best program for Twitter This has been the ...\n",
              "106      1        0  <s>Did work, but when you fix something not br...\n",
              "118      1        0  <s>Need to add more stuff Some of the stuff do...\n",
              "131      1        0  <s>In Freetown; Sierra Leone best way to stay ...\n",
              "134      0        1  <s>It works for me I use it to scan and send d...\n",
              "136      1        0  <s>Good app But I need voice call via wifi</s>...\n",
              "148      1        0  <s>I use viber every day. It's great!</s><pad>...\n",
              "150      1        0  <s>Very nice app. Saves me a bundle. Clear voi...\n",
              "152      0        1  <s>When this app first came out, it was very m...\n",
              "171      1        0  <s>Even with service, it'll still fail to send...\n",
              "175      0        1  <s>Better than Twitter app Decent but gaudy</s...\n",
              "180      0        1  <s>OneNote is an indispensable app. Combine it..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3cfae62f-0659-4129-95bd-c5de98b18f52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>targets</th>\n",
              "      <th>input_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;This app is wonderful! Quite a lifesaver. T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;With iPhone 5 works just perfectly&lt;/s&gt;&lt;pad&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Pac-Man 256 is now one of my favorite games...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;It's ok. If you're going to charge for an a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;5 estrellas o can talk we my love one and s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;Great app. Love that I can scan several dif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;Love this app and it's gotten even better w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;I run GTD on my iPhone using the one note a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Phenomenal!One of my most used apps. I have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;Best program for Twitter This has been the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Did work, but when you fix something not br...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Need to add more stuff Some of the stuff do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;In Freetown; Sierra Leone best way to stay ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;It works for me I use it to scan and send d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Good app But I need voice call via wifi&lt;/s&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;I use viber every day. It's great!&lt;/s&gt;&lt;pad&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Very nice app. Saves me a bundle. Clear voi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;When this app first came out, it was very m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>&lt;s&gt;Even with service, it'll still fail to send...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;Better than Twitter app Decent but gaudy&lt;/s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;s&gt;OneNote is an indispensable app. Combine it...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cfae62f-0659-4129-95bd-c5de98b18f52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3cfae62f-0659-4129-95bd-c5de98b18f52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3cfae62f-0659-4129-95bd-c5de98b18f52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gm371szF4Ocv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}